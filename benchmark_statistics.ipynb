{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising with the deep decoder\n",
    "\n",
    "The code below demonstrates the denoising performance on an example image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib notebook\n",
    "\n",
    "import glob\n",
    "import heapq\n",
    "import os\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "import random\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from include import *\n",
    "from PIL import Image\n",
    "import PIL\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import torch\n",
    "import torch.optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Make experiments reproducible with deterministic seed.\n",
    "torch.manual_seed(1)\n",
    "\n",
    "GPU = False\n",
    "if GPU == True:\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.cuda.manual_seed(1)\n",
    "    torch.cuda.empty_cache()\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    print(\"num GPUs\",torch.cuda.device_count())\n",
    "else:\n",
    "    dtype = torch.FloatTensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark\n",
    "\n",
    "Load the list of benchmark images from the paper:\n",
    "  \n",
    "  ```\n",
    "  Jun Xu, Hui Li, Zhetong Liang, David Zhang, and Lei Zhang\n",
    "  Real-world Noisy Image Denoising: A New Benchmark\n",
    "  https://arxiv.org/abs/1804.02603, 2018.\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_images(benchmark_dir = \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/\"):\n",
    "    clean_suffix = '_mean.JPG'\n",
    "    noisy_suffix = '_real.JPG'\n",
    "    benchmark_clean_images = sorted(glob.glob(benchmark_dir + '*' + clean_suffix))\n",
    "    benchmark_noisy_images = sorted(glob.glob(benchmark_dir + '*' + noisy_suffix))\n",
    "    benchmark_pairs = []\n",
    "    for clean_image_path in benchmark_clean_images:\n",
    "        root_filename = clean_image_path[:-len(clean_suffix)]\n",
    "        noisy_image_path = root_filename + noisy_suffix\n",
    "        assert noisy_image_path in benchmark_noisy_images\n",
    "        benchmark_pairs.append({\"clean\": clean_image_path, \"noisy\": noisy_image_path})\n",
    "    return benchmark_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 benchmark pairs.\n",
      "(\"Example: ['0: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon5D2_5_160_3200_chair_11_mean.JPG', \"\n",
      " \"'1: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon5D2_5_160_3200_chair_14_mean.JPG', \"\n",
      " \"'2: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon5D2_5_160_3200_chair_5_mean.JPG', \"\n",
      " \"'3: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon5D2_5_160_3200_plug_11_mean.JPG', \"\n",
      " \"'4: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon5D2_5_160_3200_plug_12_mean.JPG', \"\n",
      " \"'5: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon5D2_5_160_6400_bicycle_10_mean.JPG', \"\n",
      " \"'6: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon5D2_5_160_6400_bicycle_6_mean.JPG', \"\n",
      " \"'7: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon5D2_5_160_6400_bicycle_8_mean.JPG', \"\n",
      " \"'8: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon5D2_5_160_6400_circuit_11_mean.JPG', \"\n",
      " \"'9: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon5D2_5_160_6400_circuit_3_mean.JPG', \"\n",
      " \"'10: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon5D2_5_160_6400_circuit_8_mean.JPG', \"\n",
      " \"'11: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon5D2_5_160_6400_desk_10_mean.JPG', \"\n",
      " \"'12: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon5D2_5_160_6400_desk_4_mean.JPG', \"\n",
      " \"'13: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon5D2_5_160_6400_desk_5_mean.JPG', \"\n",
      " \"'14: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon5D2_5_160_6400_desk_7_mean.JPG', \"\n",
      " \"'15: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon5D2_5_160_6400_reciever_13_mean.JPG', \"\n",
      " \"'16: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon5D2_5_160_6400_reciever_1_mean.JPG', \"\n",
      " \"'17: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon5D2_5_160_6400_reciever_2_mean.JPG', \"\n",
      " \"'18: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon5D2_5_160_6400_reciever_4_mean.JPG', \"\n",
      " \"'19: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon5D2_5_160_6400_reciever_6_mean.JPG', \"\n",
      " \"'20: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon5D2_5_160_6400_reciever_8_mean.JPG', \"\n",
      " \"'21: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon5D2_5_200_3200_fruit_11_mean.JPG', \"\n",
      " \"'22: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon5D2_5_200_3200_fruit_12_mean.JPG', \"\n",
      " \"'23: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon5D2_5_200_3200_fruit_14_mean.JPG', \"\n",
      " \"'24: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon5D2_5_200_3200_toy_12_mean.JPG', \"\n",
      " \"'25: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon5D2_5_200_3200_toy_1_mean.JPG', \"\n",
      " \"'26: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon5D2_5_200_3200_toy_2_mean.JPG', \"\n",
      " \"'27: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon5D2_5_200_3200_toy_3_mean.JPG', \"\n",
      " \"'28: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon5D2_5_200_3200_toy_6_mean.JPG', \"\n",
      " \"'29: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon600D_3-5_125_1600_waterhouse_10_mean.JPG', \"\n",
      " \"'30: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon600D_3-5_125_1600_waterhouse_11_mean.JPG', \"\n",
      " \"'31: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon600D_3-5_125_1600_waterhouse_12_mean.JPG', \"\n",
      " \"'32: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon600D_4-5_125_1600_book_11_mean.JPG', \"\n",
      " \"'33: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon600D_4-5_125_1600_book_13_mean.JPG', \"\n",
      " \"'34: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon600D_4-5_125_1600_toy_11_mean.JPG', \"\n",
      " \"'35: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon600D_4-5_125_1600_toy_13_mean.JPG', \"\n",
      " \"'36: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon600D_4-5_125_1600_toy_16_mean.JPG', \"\n",
      " \"'37: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon600D_4-5_125_1600_toy_1_mean.JPG', \"\n",
      " \"'38: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon600D_4-5_125_1600_toy_2_mean.JPG', \"\n",
      " \"'39: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon600D_4-5_125_1600_toy_3_mean.JPG', \"\n",
      " \"'40: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon80D_8_8_12800_printer_11_mean.JPG', \"\n",
      " \"'41: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon80D_8_8_12800_printer_14_mean.JPG', \"\n",
      " \"'42: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon80D_8_8_12800_printer_15_mean.JPG', \"\n",
      " \"'43: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon80D_8_8_12800_printer_16_mean.JPG', \"\n",
      " \"'44: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon80D_8_8_3200_ball_16_mean.JPG', \"\n",
      " \"'45: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon80D_8_8_3200_ball_1_mean.JPG', \"\n",
      " \"'46: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon80D_8_8_3200_ball_2_mean.JPG', \"\n",
      " \"'47: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon80D_8_8_3200_ball_6_mean.JPG', \"\n",
      " \"'48: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon80D_8_8_3200_ball_7_mean.JPG', \"\n",
      " \"'49: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon80D_8_8_6400_comproom_11_mean.JPG', \"\n",
      " \"'50: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon80D_8_8_6400_comproom_12_mean.JPG', \"\n",
      " \"'51: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon80D_8_8_6400_comproom_13_mean.JPG', \"\n",
      " \"'52: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon80D_8_8_6400_comproom_14_mean.JPG', \"\n",
      " \"'53: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon80D_8_8_6400_comproom_16_mean.JPG', \"\n",
      " \"'54: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Canon80D_8_8_800_GO_11_mean.JPG', \"\n",
      " \"'55: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/NikonD800_10_100_6400_planandsofa_2_mean.JPG', \"\n",
      " \"'56: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/NikonD800_10_100_6400_planandsofa_7_mean.JPG', \"\n",
      " \"'57: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/NikonD800_11_160_3200_classroom_15_mean.JPG', \"\n",
      " \"'58: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/NikonD800_11_160_3200_classroom_4_mean.JPG', \"\n",
      " \"'59: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/NikonD800_11_160_3200_classroom_6_mean.JPG', \"\n",
      " \"'60: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/NikonD800_4-5_160_1800_classroom_15_mean.JPG', \"\n",
      " \"'61: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/NikonD800_4-5_160_1800_classroom_5_mean.JPG', \"\n",
      " \"'62: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/NikonD800_4-5_160_1800_classroom_9_mean.JPG', \"\n",
      " \"'63: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/NikonD800_5-6_160_6400_wall_15_mean.JPG', \"\n",
      " \"'64: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/NikonD800_5-6_160_6400_wall_2_mean.JPG', \"\n",
      " \"'65: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/NikonD800_5-6_160_6400_wall_6_mean.JPG', \"\n",
      " \"'66: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/NikonD800_5-6_160_6400_wall_9_mean.JPG', \"\n",
      " \"'67: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/NikonD800_5_100_4000_flower_12_mean.JPG', \"\n",
      " \"'68: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/NikonD800_5_100_4000_flower_13_mean.JPG', \"\n",
      " \"'69: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/NikonD800_5_100_4000_flower_14_mean.JPG', \"\n",
      " \"'70: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/NikonD800_5_100_4000_flower_15_mean.JPG', \"\n",
      " \"'71: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/NikonD800_5_100_4000_flower_1_mean.JPG', \"\n",
      " \"'72: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/NikonD800_5_100_4000_flower_2_mean.JPG', \"\n",
      " \"'73: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/NikonD800_5_125_6400_stair_10_mean.JPG', \"\n",
      " \"'74: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/NikonD800_5_125_6400_stair_1_mean.JPG', \"\n",
      " \"'75: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/NikonD800_5_125_6400_stair_3_mean.JPG', \"\n",
      " \"'76: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/NikonD800_5_125_6400_stair_6_mean.JPG', \"\n",
      " \"'77: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/NikonD800_5_125_6400_stair_7_mean.JPG', \"\n",
      " \"'78: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/NikonD800_6-3_125_5000_plant_10_mean.JPG', \"\n",
      " \"'79: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/NikonD800_6-3_125_5000_plant_1_mean.JPG', \"\n",
      " \"'80: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/NikonD800_6-3_125_5000_plant_3_mean.JPG', \"\n",
      " \"'81: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/NikonD800_6-3_125_5000_plant_6_mean.JPG', \"\n",
      " \"'82: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/NikonD800_6-3_125_5000_plant__4_mean.JPG', \"\n",
      " \"'83: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/NikonD800_8_100_6400_bulletin_3_mean.JPG', \"\n",
      " \"'84: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/NikonD800_8_100_6400_bulletin_5_mean.JPG', \"\n",
      " \"'85: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/NikonD800_8_100_6400_bulletin_7_mean.JPG', \"\n",
      " \"'86: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/NikonD800_8_100_6400_bulletin_8_mean.JPG', \"\n",
      " \"'87: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/NikonD800_8_125_6400_photo_19_mean.JPG', \"\n",
      " \"'88: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Sony_3-5_200_1600_classroom_10_mean.JPG', \"\n",
      " \"'89: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Sony_3-5_200_1600_classroom_13_mean.JPG', \"\n",
      " \"'90: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Sony_3-5_200_1600_classroom_14_mean.JPG', \"\n",
      " \"'91: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Sony_4-5_125_1600_toy_10_mean.JPG', \"\n",
      " \"'92: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Sony_4-5_125_1600_toy_11_mean.JPG', \"\n",
      " \"'93: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Sony_4-5_125_3200_plant_10_mean.JPG', \"\n",
      " \"'94: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Sony_4-5_125_3200_plant_11_mean.JPG', \"\n",
      " \"'95: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Sony_4-5_125_3200_plant_13_mean.JPG', \"\n",
      " \"'96: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Sony_4-5_125_3200_plant_14_mean.JPG', \"\n",
      " \"'97: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Sony_4-5_125_6400_waterhouse_10_mean.JPG', \"\n",
      " \"'98: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Sony_4_200_3200_door_10_mean.JPG', \"\n",
      " \"'99: \"\n",
      " \"./test_data/PolyU-Real-World-Noisy-Images-Dataset-master/CroppedImages/Sony_4_200_3200_door_11_mean.JPG']\")\n"
     ]
    }
   ],
   "source": [
    "BENCHMARK_IMAGES = benchmark_images()\n",
    "print(\"Loaded %d benchmark pairs.\" % len(BENCHMARK_IMAGES))\n",
    "pprint(\"Example: %s\" % [\"%d: %s\" % (k,v['clean']) for k,v in enumerate(BENCHMARK_IMAGES)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_statistics():\n",
    "    snrs = []\n",
    "    ssims = []\n",
    "    durations = []\n",
    "    for benchmark_number in range(len(BENCHMARK_IMAGES)):\n",
    "        existing_picklefiles = glob.glob(\"benchmark_results/%d_*_benchmark_result.p\" % benchmark_number)\n",
    "        if len(existing_picklefiles) > 0:\n",
    "            existing_picklefile = existing_picklefiles[0]\n",
    "            print(\"Found existing stored result %s\" % existing_picklefile)\n",
    "            print(\"Displaying...\")\n",
    "            snr, ssim, duration, img_clean_np, img_noisy_np, img_out_np = pickle.load(open(existing_picklefile, \"rb\"))\n",
    "            snrs.append(snr)\n",
    "            ssims.append(ssim)\n",
    "            durations.append(durations)\n",
    "    print(stats.describe(snrs))\n",
    "    print(stats.describe(ssims))\n",
    "    print(stats.describe(durations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing stored result benchmark_results/0_33.954788653448304_benchmark_result.p\n",
      "Displaying...\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "bad pickle data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/.virtualenvs/cs230/lib/python3.5/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cs230/lib/python3.5/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    546\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cs230/lib/python3.5/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    512\u001b[0m                 deserialized_objects[root_key] = restore_location(\n\u001b[0;32m--> 513\u001b[0;31m                     data_type(size), location)\n\u001b[0m\u001b[1;32m    514\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mroot_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cs230/lib/python3.5/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cs230/lib/python3.5/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             raise RuntimeError('Attempting to deserialize object on a CUDA '\n\u001b[0m\u001b[1;32m     76\u001b[0m                                \u001b[0;34m'device but torch.cuda.is_available() is False. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location='cpu' to map your storages to the CPU.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1b611c1c011d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbenchmark_statistics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-2e3b2f7a8c0c>\u001b[0m in \u001b[0;36mbenchmark_statistics\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Found existing stored result %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mexisting_picklefile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Displaying...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0msnr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mssim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mduration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_clean_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_noisy_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_out_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexisting_picklefile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0msnrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msnr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mssims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mssim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cs230/lib/python3.5/site-packages/torch/storage.py\u001b[0m in \u001b[0;36m_load_from_bytes\u001b[0;34m(b)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_load_from_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cs230/lib/python3.5/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cs230/lib/python3.5/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    535\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m     \u001b[0mmagic_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmagic_number\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mMAGIC_NUMBER\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid magic number; corrupt file?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: bad pickle data"
     ]
    }
   ],
   "source": [
    "benchmark_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
